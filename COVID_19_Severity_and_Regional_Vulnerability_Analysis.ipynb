{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYSODT9cw+SrNOdFVsxl2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giftsboard/DataAnalysisProject/blob/main/COVID_19_Severity_and_Regional_Vulnerability_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Objective (Restated Clearly)**\n",
        "\n",
        "We want to:\n",
        "\n",
        "1. Measure observed COVID severity at province level\n",
        "\n",
        "2. Estimate expected severity based on demographics & health capacity\n",
        "\n",
        "3. Calculate excess severity (residuals)\n",
        "\n",
        "4. Test whether macro-financial vulnerability explains excess severity\n",
        "\n",
        "5. Visualise and classify provinces into risk typologies"
      ],
      "metadata": {
        "id": "jEmfiHB72xAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries & Load Data\n",
        "\n",
        "* pandas / numpy for data manipulation\n",
        "\n",
        "* statsmodels for interpretable regression\n",
        "\n",
        "* sklearn for clustering & scaling\n",
        "\n",
        "* matplotlib / seaborn for publication-quality visuals"
      ],
      "metadata": {
        "id": "qMkCuJ-k5M1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "nhvcZjHO5EcZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loading**"
      ],
      "metadata": {
        "id": "WtLUNj2j4scS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C5GanCcMAcN0",
        "outputId": "d0855c89-4e9d-47d1-d178-2cb27bff69f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   2019_gdp_per_capita_(thousands)  2019_population  \\\n",
            "0                              NaN              NaN   \n",
            "1                              NaN              NaN   \n",
            "2                              NaN              NaN   \n",
            "3                              NaN              NaN   \n",
            "4                              NaN              NaN   \n",
            "\n",
            "   2019_real_gdp_(thousands)  2020_real_gdp_(millions)  activos  cases_14days  \\\n",
            "0                        NaN                       NaN      NaN           NaN   \n",
            "1                        NaN                       NaN      NaN           NaN   \n",
            "2                        NaN                       NaN      NaN           NaN   \n",
            "3                        NaN                       NaN      NaN           NaN   \n",
            "4                        NaN                       NaN      NaN           NaN   \n",
            "\n",
            "   cases_7days  cases_accumulated  cases_accumulated_pcr  cases_pcr_14days  \\\n",
            "0          NaN                NaN                    NaN               NaN   \n",
            "1          NaN                NaN                    NaN               NaN   \n",
            "2          NaN                NaN                    NaN               NaN   \n",
            "3          NaN                NaN                    NaN               NaN   \n",
            "4          NaN                NaN                    NaN               NaN   \n",
            "\n",
            "   ...  stockstestmax_thous  temp_jan_2020  temperature  test_13mar  \\\n",
            "0  ...             0.822333          -0.46          NaN         NaN   \n",
            "1  ...             1.068667           5.50          NaN         NaN   \n",
            "2  ...             2.663000           0.42          NaN         NaN   \n",
            "3  ...             0.461667           2.01          NaN         NaN   \n",
            "4  ...             0.273841          10.96          NaN         NaN   \n",
            "\n",
            "   test_27mar  test_3050 test_5070 test_7090  test_90110  testac  \n",
            "0         NaN        NaN       NaN       NaN         NaN     NaN  \n",
            "1         NaN        NaN       NaN       NaN         NaN     NaN  \n",
            "2         NaN        NaN       NaN       NaN         NaN     NaN  \n",
            "3         NaN        NaN       NaN       NaN         NaN     NaN  \n",
            "4         NaN        NaN       NaN       NaN         NaN     NaN  \n",
            "\n",
            "[5 rows x 98 columns]\n",
            "['2019_gdp_per_capita_(thousands)', '2019_population', '2019_real_gdp_(thousands)', '2020_real_gdp_(millions)', 'activos', 'cases_14days', 'cases_7days', 'cases_accumulated', 'cases_accumulated_pcr', 'cases_pcr_14days', 'cases_pcr_7days', 'cases_per_cienmil', 'cb_100thous2007', 'cds_growth', 'changecb_100thous', 'chpasto_perc', 'comments', 'country', 'covid_100', 'covid_13mar', 'covid_27mar', 'covid_3050', 'covid_5070', 'covid_7090', 'covid_90110', 'covid_measure', 'csm_gap_gdp_potgdp', 'csm_gap_gdp_potgdp_long', 'csm_growthcds', 'csm_growthrealgdp', 'daily_cases', 'daily_cases_avg7', 'daily_cases_pcr', 'daily_cases_pcr_avg7', 'daily_deaths', 'daily_deaths_avg3', 'daily_deaths_avg7', 'daily_deaths_inc', 'date', 'deathrate_thous2018', 'deaths_last_week', 'debt_growth_07-2010', 'debt_growth_07-2011', 'debt_growth_07-2012', 'debt_growth_07-2013', 'deceased', 'deceassed_per_100000', 'gdpcapita_eurothous2019', 'geo_type', 'goveff_ch1907', 'goveffectiv_2019', 'health-care_exp_2007-2019', 'health-care_exp_2008-2019', 'health-care_exp_2009-2019', 'health-care_exp_2010-2019', 'health-care_exp_2011-2019', 'hospitalized', 'hospitalized_per_100000', 'icubeds_1', 'icubeds_2']\n",
            "(10448, 98)\n",
            "                                col  present\n",
            "0                              date     True\n",
            "1                          geo_type     True\n",
            "2                           country     True\n",
            "3                            region     True\n",
            "4                          province     True\n",
            "5                  daily_cases_avg7     True\n",
            "6                 daily_deaths_avg7     True\n",
            "7              deceassed_per_100000     True\n",
            "8                      hospitalized     True\n",
            "9                    intensive_care     True\n",
            "10                  2019_population     True\n",
            "11            median_population_age     True\n",
            "12                          obesity     True\n",
            "13                 pop_density_2019     True\n",
            "14  2019_gdp_per_capita_(thousands)     True\n"
          ]
        }
      ],
      "source": [
        "# Load Excel file (df) and do a quick structural scan to find key columns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "xlsx_path = 'unified_gfc_covid_hybrid.xlsx'\n",
        "\n",
        "df = pd.read_excel(xlsx_path)\n",
        "\n",
        "print(df.head())\n",
        "print(df.columns.tolist()[:60])\n",
        "print(df.shape)\n",
        "\n",
        "# Basic column presence checks\n",
        "key_checks = ['date','geo_type','country','region','province','daily_cases_avg7','daily_deaths_avg7','deceassed_per_100000',\n",
        "              'hospitalized','intensive_care','2019_population','median_population_age','obesity','pop_density_2019',\n",
        "              '2019_gdp_per_capita_(thousands)']\n",
        "col_presence = pd.DataFrame({'col': key_checks, 'present': [c in df.columns for c in key_checks]})\n",
        "print(col_presence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning, Manipulation, Filtering & Standardisation**"
      ],
      "metadata": {
        "id": "tl90hhLo_SXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic data inspection plus focus on cleaning pipeline for province-level analysis.\n",
        "\n",
        "Cleaned and structured dataset, focusing on province-level data by standardizing columns, parsing dates, and filtering relevant information.\n",
        "- Created a copy of the raw DataFrame.\n",
        "- Standardized string columns by stripping whitespace and replacing 'nan' and 'None' with NaN.\n",
        "- Parsed the 'date' column to datetime format.\n",
        "- Filtered the DataFrame to retain only province-level data if available.\n",
        "- Built a list of columns to keep based on identifiers, COVID outcomes, structural controls, and financial vulnerability proxies.\n",
        "- Coerced non-identifier columns to numeric types.\n",
        "- Dropped rows without key identifiers and without dates.\n",
        "- Removed duplicate rows."
      ],
      "metadata": {
        "id": "OiaXwwAvHls6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = df.copy()"
      ],
      "metadata": {
        "id": "Z4wYxXu4IhrJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize obvious string columns\n",
        "for col_name in ['geo_type','province','region','country','state','state_name','state_full','state_code','source','source_name']:\n",
        "    if col_name in raw_df.columns:\n",
        "        raw_df[col_name] = raw_df[col_name].astype(str).str.strip().replace({'nan': np.nan, 'None': np.nan})"
      ],
      "metadata": {
        "id": "xGwN0xMeRqbz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse date\n",
        "if 'date' in raw_df.columns:\n",
        "    raw_df['date'] = pd.to_datetime(raw_df['date'], errors='coerce')"
      ],
      "metadata": {
        "id": "m-olNoOTRwXb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prefer province level when available\n",
        "if 'geo_type' in raw_df.columns:\n",
        "    prov_df = raw_df[raw_df['geo_type'].astype(str).str.lower().eq('province')].copy()\n",
        "else:\n",
        "    prov_df = raw_df.copy()"
      ],
      "metadata": {
        "id": "2A7xJ66IR1hp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build column allowlist by concept\n",
        "id_cols = [c for c in ['date','country','region','province','ine_code','geo_type'] if c in prov_df.columns]"
      ],
      "metadata": {
        "id": "MGPgr3kTR8Oy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covid_outcome_cols = [c for c in prov_df.columns if any(k in c.lower() for k in [\n",
        "    'daily_cases','daily_deaths','cases_','deaths_','deceased','recovered','hospitalized','intensive_care','icu','pcr','test','testac','num_casos'\n",
        "])]"
      ],
      "metadata": {
        "id": "qdP9xdjmSJt6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Structural controls\n",
        "struct_cols = [c for c in [\n",
        "    '2019_population','poblacion','popdensity2018','pop_density_2019','median_population_age','obesity',\n",
        "    'goveffectiv_2019','goveff_ch1907','deathrate_thous2018','temp_jan_2020','temperature',\n",
        "    'icubeds_1','icubeds_2','icubeds_2_pub','hospitalized_per_100000','intensive_care_per_1000000'\n",
        "] if c in prov_df.columns]"
      ],
      "metadata": {
        "id": "KraNfu0eSOOS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Financial vulnerability proxies (GFC-style)\n",
        "fin_cols = [c for c in prov_df.columns if any(k in c.lower() for k in [\n",
        "    'cds','debt_growth','cb_','changecb','csm_','gap_gdp','real_gdp_growth','stockstestmax','gdpcapita','gdp_per_capita','real_gdp'\n",
        "])]"
      ],
      "metadata": {
        "id": "Q3BhKXXXSXXq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_cols = []\n",
        "for block in [id_cols, covid_outcome_cols, struct_cols, fin_cols]:\n",
        "    for c in block:\n",
        "        if c not in keep_cols:\n",
        "            keep_cols.append(c)"
      ],
      "metadata": {
        "id": "IvF2LaFwSnGC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "focused_df = prov_df[keep_cols].copy()"
      ],
      "metadata": {
        "id": "24CBLl0jSpKC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coerce numeric columns (everything except identifiers)\n",
        "id_set = set(id_cols)\n",
        "for c in focused_df.columns:\n",
        "    if c not in id_set:\n",
        "        focused_df[c] = pd.to_numeric(focused_df[c], errors='coerce')"
      ],
      "metadata": {
        "id": "Bvf-khyATHha"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows without key identifiers\n",
        "if 'province' in focused_df.columns:\n",
        "    focused_df = focused_df[focused_df['province'].notna()].copy()"
      ],
      "metadata": {
        "id": "JtHMttIlTYfq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows without date if we want panel structure; otherwise keep latest snapshot per province later\n",
        "if 'date' in focused_df.columns:\n",
        "    focused_df = focused_df[focused_df['date'].notna()].copy()"
      ],
      "metadata": {
        "id": "GVNhla2TTip6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove exact duplicates\n",
        "focused_df = focused_df.drop_duplicates().copy()"
      ],
      "metadata": {
        "id": "xnFfgcymTxfS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet identifies the most suitable geographic level for analysis based on the availability of non-null numeric data.\n",
        "- Create a copy of the DataFrame and convert 'date' column to datetime.\n",
        "- Check for 'geo_type' column and count occurrences of each geo level.\n",
        "- Initialize a dictionary to store candidate frames based on geo levels.\n",
        "- For each geo level, filter the DataFrame and check for the presence of an entity column.\n",
        "- Count non-null numeric values in the filtered DataFrame.\n",
        "- Store the results in the candidate frames dictionary.\n",
        "- Select the geo level with the maximum number of non-null values or default to 'all' if none are found."
      ],
      "metadata": {
        "id": "idlAMOXpU7hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If province-level filtered set is empty, fall back to best available geo level with real data.\n",
        "raw_df = df.copy()\n",
        "raw_df['date'] = pd.to_datetime(raw_df['date'], errors='coerce') if 'date' in raw_df.columns else pd.NaT"
      ],
      "metadata": {
        "id": "zgpL6-2SVQhq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'geo_type' in raw_df.columns:\n",
        "    geo_counts = raw_df['geo_type'].astype(str).str.lower().value_counts(dropna=False)\n",
        "    geo_levels = geo_counts.index.tolist()\n",
        "else:\n",
        "    geo_levels = ['all']"
      ],
      "metadata": {
        "id": "Fl2xswsYVnPK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geo_level_to_use = None\n",
        "candidate_frames = {}"
      ],
      "metadata": {
        "id": "dFKVB3icVxgj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lvl in geo_levels:\n",
        "    if lvl == 'all':\n",
        "        tmp = raw_df.copy()\n",
        "    else:\n",
        "        tmp = raw_df[raw_df['geo_type'].astype(str).str.lower().eq(str(lvl))].copy()\n",
        "    # basic viability: has entity id and at least one numeric non-null outside ids\n",
        "    entity_col = 'province' if 'province' in tmp.columns else ('country' if 'country' in tmp.columns else None)\n",
        "    if entity_col is None:\n",
        "        continue\n",
        "    num_nonnull = 0\n",
        "    for c in tmp.columns:\n",
        "        if c in ['date','country','region','province','ine_code','geo_type']:\n",
        "            continue\n",
        "        num_nonnull += int(pd.to_numeric(tmp[c], errors='coerce').notna().sum())\n",
        "    candidate_frames[str(lvl)] = {'rows': int(tmp.shape[0]), 'entity_col': entity_col, 'num_nonnull': int(num_nonnull)}"
      ],
      "metadata": {
        "id": "NCmG6HIVXZzK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick level with max num_nonnull\n",
        "if len(candidate_frames) > 0:\n",
        "    geo_level_to_use = sorted(candidate_frames.items(), key=lambda kv: kv[1]['num_nonnull'], reverse=True)[0][0]\n",
        "else:\n",
        "    geo_level_to_use = 'all'\n",
        "    geo_level_to_use\n",
        "candidate_frames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MsPg_hVXKQS",
        "outputId": "18bf85eb-1314-46b6-d746-709badfa725d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'all': {'rows': 0, 'entity_col': 'province', 'num_nonnull': 0}}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data For Analysis"
      ],
      "metadata": {
        "id": "c47U98kWYonT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "work_df = df.copy()"
      ],
      "metadata": {
        "id": "ecdS7BwVY33S"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize strings\n",
        "for col_name in ['geo_type','province','region','country','state','state_name','state_full','state_code','source','source_name']:\n",
        "    if col_name in work_df.columns:\n",
        "        work_df[col_name] = work_df[col_name].astype(str).str.strip().replace({'nan': np.nan, 'None': np.nan})"
      ],
      "metadata": {
        "id": "PsSsC334Y5kS"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse date\n",
        "if 'date' in work_df.columns:\n",
        "    work_df['date'] = pd.to_datetime(work_df['date'], errors='coerce')"
      ],
      "metadata": {
        "id": "vnxiwPIKY96y"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter to best geo level discovered earlier\n",
        "if 'geo_type' in work_df.columns and geo_level_to_use != 'all':\n",
        "    analysis_df = work_df[work_df['geo_type'].astype(str).str.lower().eq(geo_level_to_use)].copy()\n",
        "else:\n",
        "    analysis_df = work_df.copy()"
      ],
      "metadata": {
        "id": "7Rw7JpfOZHQa"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define allowlists for the structured analysis\n",
        "id_cols = [c for c in ['date','country','region','province','ine_code','geo_type'] if c in analysis_df.columns]"
      ],
      "metadata": {
        "id": "aB0kgTkNZJv6"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outcomes and inputs for COVID severity construction\n",
        "covid_cols = [c for c in [\n",
        "    'daily_cases','daily_cases_avg7','daily_deaths','daily_deaths_avg7','daily_deaths_avg3','daily_deaths_inc',\n",
        "    'cases_7days','cases_14days','cases_per_cienmil','cases_accumulated','cases_accumulated_pcr',\n",
        "    'cases_pcr_7days','cases_pcr_14days','pcr','testac',\n",
        "    'hospitalized','intensive_care','recovered','deceased','deceassed_per_100000',\n",
        "    'num_casos','num_casos_prueba_pcr','num_casos_prueba_test_ac'\n",
        "] if c in analysis_df.columns]"
      ],
      "metadata": {
        "id": "1bhyiaVLZXLj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Structural controls\n",
        "struct_cols = [c for c in [\n",
        "    '2019_population','poblacion','pop_density_2019','popdensity2018','median_population_age','obesity',\n",
        "    'goveffectiv_2019','goveff_ch1907','deathrate_thous2018','temperature','temp_jan_2020',\n",
        "    'icubeds_1','icubeds_2','icubeds_2_pub','hospitalized_per_100000','intensive_care_per_1000000',\n",
        "    '2019_gdp_per_capita_(thousands)','gdpcapita_eurothous2019'\n",
        "] if c in analysis_df.columns]\n"
      ],
      "metadata": {
        "id": "9Zpj4N4FZm77"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Financial vulnerability proxies\n",
        "fin_cols = [c for c in [\n",
        "    'cb_100thous2007','changecb_100thous','cds_growth',\n",
        "    'debt_growth_07-2010','debt_growth_07-2011','debt_growth_07-2012','debt_growth_07-2013',\n",
        "    'csm_gap_gdp_potgdp','csm_gap_gdp_potgdp_long','csm_growthcds','csm_growthrealgdp',\n",
        "    'real_gdp_growth_2007:q4_-_2009:q2(in%)','real_gdp_growth_2008:q1_-_2009:q2_(in%)'\n",
        "] if c in analysis_df.columns]"
      ],
      "metadata": {
        "id": "O5m1FqrfZzhD"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_cols = []\n",
        "for block in [id_cols, covid_cols, struct_cols, fin_cols]:\n",
        "    for c in block:\n",
        "        if c not in keep_cols:\n",
        "            keep_cols.append(c)"
      ],
      "metadata": {
        "id": "W7zz0HiRZ9e7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_df = analysis_df[keep_cols].copy()\n"
      ],
      "metadata": {
        "id": "Omoj-NOHaAza"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coerce numeric for non-id\n",
        "id_set = set(id_cols)\n",
        "for c in analysis_df.columns:\n",
        "    if c not in id_set:\n",
        "        analysis_df[c] = pd.to_numeric(analysis_df[c], errors='coerce')"
      ],
      "metadata": {
        "id": "Yuq7pdkraFFT"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Harmonize population\n",
        "if '2019_population' in analysis_df.columns:\n",
        "    analysis_df['pop'] = analysis_df['2019_population']\n",
        "elif 'poblacion' in analysis_df.columns:\n",
        "    analysis_df['pop'] = analysis_df['poblacion']\n",
        "else:\n",
        "    analysis_df['pop'] = np.nan"
      ],
      "metadata": {
        "id": "7U2UOFFIaRfj"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_df['pop'] = pd.to_numeric(analysis_df['pop'], errors='coerce')\n",
        "\n",
        "# Initialize base filter for 'province'\n",
        "filter_condition = analysis_df['province'].notna()\n",
        "\n",
        "# Add 'date' filter only if the 'date' column exists\n",
        "if 'date' in analysis_df.columns:\n",
        "    filter_condition = filter_condition & analysis_df['date'].notna()\n",
        "\n",
        "analysis_df = analysis_df[filter_condition].copy()\n",
        "analysis_df = analysis_df[(analysis_df['pop'].isna()) | (analysis_df['pop'] > 0)].copy()"
      ],
      "metadata": {
        "id": "7fOaikg3aed7"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with no COVID info at all (to keep real numbers)\n",
        "covid_nonnull = None\n",
        "if len(covid_cols) > 0:\n",
        "    covid_nonnull = analysis_df[covid_cols].notna().any(axis=1)\n",
        "    analysis_df = analysis_df[covid_nonnull].copy()"
      ],
      "metadata": {
        "id": "ywqLLSXRa957"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep 2020-2021 window (typical for COVID severity waves)\n",
        "if 'date' in analysis_df.columns and not analysis_df.empty:\n",
        "    analysis_df = analysis_df[(analysis_df['date'] >= pd.Timestamp('2020-01-01')) & (analysis_df['date'] <= pd.Timestamp('2021-12-31'))].copy()"
      ],
      "metadata": {
        "id": "I45CsAMvbRlE"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates per province-date by taking max of numeric fields (common in merged sources)\n",
        "numeric_cols = [c for c in analysis_df.columns if c not in id_set and c not in ['pop']]\n",
        "agg_map = {c: 'max' for c in numeric_cols}\n",
        "for c in id_cols:\n",
        "    agg_map[c] = 'first'\n",
        "agg_map['pop'] = 'max'"
      ],
      "metadata": {
        "id": "iuahL1Ixeahz"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_df = (analysis_df.sort_values('date')\n",
        "               .groupby(['province','date'], dropna=False, as_index=False)\n",
        "               .agg(agg_map))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "qct6Se1ZeiUK",
        "outputId": "56089513-c984-4a74-8b87-a9cb61abd41e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1352794848.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m analysis_df = (analysis_df.sort_values('date')\n\u001b[0m\u001b[1;32m      2\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'province'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                .agg(agg_map))\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7187\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7189\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7191\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'date'"
          ]
        }
      ]
    }
  ]
}